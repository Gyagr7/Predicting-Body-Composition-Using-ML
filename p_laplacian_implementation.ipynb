{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b19e744a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "p-Laplacian regression (GraphLearning) â€” basic experiment runner\n",
    "\n",
    "Installs (recommended):\n",
    "    pip install numpy pandas scikit-learn graphlearning\n",
    "\n",
    "Optional (only if you later add plots):\n",
    "    pip install matplotlib\n",
    "\n",
    "Notes:\n",
    "- This script evaluates p-Laplacian regression using a *20% labeled / 80% unlabeled* split.\n",
    "- We use KFold to generate 5 disjoint 20% folds; each fold is treated as the labeled set.\n",
    "- Features are standardized using ONLY the labeled set (to avoid leakage).\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import graphlearning as gl\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# User configuration\n",
    "# ----------------------------\n",
    "FILE_PATHS = {\n",
    "    \"Male Data\": \"/Users/gyaneshwaragrahari/Documents/GitHub/Pennington_Project/Data/male.csv\",\n",
    "    \"Female Data\": \"/Users/gyaneshwaragrahari/Documents/GitHub/Pennington_Project/Data/female.csv\",\n",
    "    \"Complete Data\": \"/Users/gyaneshwaragrahari/Documents/GitHub/Pennington_Project/Data/complete_data.csv\",\n",
    "}\n",
    "\n",
    "EXCLUDED_COLUMNS = [\"0\", \"PPT ID\", \"Site\", \"Gender\", \"BMD - Total\", \"ALM\", \"% fat - Total\", \"Race\"]\n",
    "TARGET_COLUMNS = [\"ALM\", \"% fat - Total\", \"BMD - Total\"]\n",
    "\n",
    "K_VALUES = [10, 30, 50]\n",
    "P_VALUES = [2, 5, 10, 100, 500, 1000, 5000, 10000, 50000, 100000]\n",
    "\n",
    "N_SPLITS = 5   # KFold: each fold is 20% of the data\n",
    "N_RUNS = 10    # independent shuffles (random_state = run)\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Helpers\n",
    "# ----------------------------\n",
    "def load_xy(csv_path: str, excluded_columns: list[str], target_column: str) -> tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Load CSV, return (X, y).\"\"\"\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    missing = [c for c in excluded_columns + [target_column] if c not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Missing columns in {csv_path}: {missing}\")\n",
    "\n",
    "    X = df.drop(columns=excluded_columns).to_numpy(dtype=float)\n",
    "    y = df[target_column].to_numpy(dtype=float)\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def relative_rmse(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Relative RMSE (%).\n",
    "    Uses RMS(y_true) in the denominator (common for normalized RMSE).\n",
    "    \"\"\"\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    denom = np.sqrt(np.mean(y_true**2)) + 1e-12  # avoid division by 0\n",
    "    return 100.0 * rmse / denom\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Main experiment\n",
    "# ----------------------------\n",
    "def main() -> dict:\n",
    "    # rmse_values[target][dataset] = array shape (len(K_VALUES), len(P_VALUES))\n",
    "    rmse_values: dict[str, dict[str, np.ndarray]] = {\n",
    "        target: {ds: np.zeros((len(K_VALUES), len(P_VALUES)), dtype=float) for ds in FILE_PATHS}\n",
    "        for target in TARGET_COLUMNS\n",
    "    }\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    for target in TARGET_COLUMNS:\n",
    "        for dataset_name, csv_path in FILE_PATHS.items():\n",
    "            X, y = load_xy(csv_path, EXCLUDED_COLUMNS, target)\n",
    "\n",
    "            for run in range(N_RUNS):\n",
    "                kf = KFold(n_splits=N_SPLITS, shuffle=True, random_state=run)\n",
    "\n",
    "                for labeled_idx, unlabeled_idx in kf.split(X):\n",
    "                    # By convention, we treat the 20% fold as the *labeled* set\n",
    "                    X_labeled, y_labeled = X[labeled_idx], y[labeled_idx]\n",
    "                    X_unlabeled, y_unlabeled = X[unlabeled_idx], y[unlabeled_idx]\n",
    "\n",
    "                    # Standardize using ONLY labeled data (prevents leakage)\n",
    "                    X_labeled = scaler.fit_transform(X_labeled)\n",
    "                    X_unlabeled = scaler.transform(X_unlabeled)\n",
    "\n",
    "                    # Combine for graph construction (GraphLearning expects full dataset)\n",
    "                    X_all = np.vstack([X_labeled, X_unlabeled])\n",
    "                    y_all = np.concatenate([y_labeled, y_unlabeled])\n",
    "\n",
    "                    labeled_nodes = np.arange(len(y_labeled))  # labeled nodes come first\n",
    "                    unlabeled_mask = np.zeros(len(y_all), dtype=bool)\n",
    "                    unlabeled_mask[len(y_labeled):] = True\n",
    "\n",
    "                    for i, k in enumerate(K_VALUES):\n",
    "                        W = gl.weightmatrix.knn(X_all, k)\n",
    "                        G = gl.graph(W)\n",
    "\n",
    "                        for j, p in enumerate(P_VALUES):\n",
    "                            # p-Laplace regression: returns predictions on ALL nodes\n",
    "                            y_hat = G.plaplace(labeled_nodes, y_labeled, p)\n",
    "\n",
    "                            score = relative_rmse(y_true=y_all[unlabeled_mask],\n",
    "                                                  y_pred=y_hat[unlabeled_mask])\n",
    "                            rmse_values[target][dataset_name][i, j] += score\n",
    "\n",
    "            # Average over folds * runs\n",
    "            rmse_values[target][dataset_name] /= (N_SPLITS * N_RUNS)\n",
    "\n",
    "    return rmse_values\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    results = main()\n",
    "\n",
    "    # Minimal printing; you can replace this with saving to CSV/LaTeX.\n",
    "    for target, ds_map in results.items():\n",
    "        print(f\"\\n=== Target: {target} ===\")\n",
    "        for ds_name, arr in ds_map.items():\n",
    "            print(f\"\\n{ds_name} (rows=k, cols=p):\")\n",
    "            print(arr)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
